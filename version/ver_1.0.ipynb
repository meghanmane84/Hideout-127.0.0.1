{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MybG5OIVxcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4687bd3a-96c1-4db8-fcb5-69ffaa46641b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU is available: \", tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "id": "uTNMVtFzblqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047ffada-e55e-4572-a714-15b0fe35ae53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "L5_wdK7OfXQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_path=\"/content/sample\""
      ],
      "metadata": {
        "id": "aFVdD6-3lnbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the image files in the directory\n",
        "image_files = [file for file in os.listdir(sample_path) if file.endswith('.jpg')]\n",
        "\n",
        "# Create an empty list to store pixel data and timestamps\n",
        "pixel_data = []\n",
        "timestamps = []\n",
        "\n",
        "# Iterate through the image files, read each image, resize to target size, and flatten as 1D array\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(sample_path, image_file)\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Load image with alpha channel\n",
        "\n",
        "    # Extract the timestamp from the image filename (assuming the format is \"YYYYMMDD_HHMM.jpg\")\n",
        "    filename_without_extension = os.path.splitext(image_file)[0]\n",
        "    timestamp = filename_without_extension  # Use the entire filename as the timestamp\n",
        "\n",
        "    # Resize the image to the target size (256x256)\n",
        "    target_size = (256, 256)\n",
        "    resized_image = cv2.resize(image, target_size)\n",
        "\n",
        "    # Flatten the 2D array to 1D and normalize pixel values by dividing by 255\n",
        "    flattened_pixels = (resized_image.astype(np.float32) / 255.0).flatten()\n",
        "\n",
        "    # Append the flattened pixels and timestamp to their respective lists\n",
        "    pixel_data.append(flattened_pixels)\n",
        "    timestamps.append(timestamp)\n",
        "\n",
        "# Create a Pandas DataFrame from the pixel data and timestamps\n",
        "df = pd.DataFrame(pixel_data)\n",
        "\n",
        "# Convert the timestamps to datetime objects and set them as the index\n",
        "df.index = pd.to_datetime(timestamps, format='%Y%m%d_%H%M')\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "csv_path = \"/content/sample_image_data_with_timestamps.csv\"\n",
        "df.to_csv(csv_path)\n",
        "\n",
        "# Print the path to the saved CSV file\n",
        "csv_path"
      ],
      "metadata": {
        "id": "sUOQfrYouns_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/'\n",
        "os.chdir(data_dir)"
      ],
      "metadata": {
        "id": "n2691FhXvIYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "vzdy3DMUvcvt",
        "outputId": "7dcf7312-89e2-4c09-cae0-29be64f3582b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       0         1         2         3         4       \\\n",
              "2023-09-17 12:00:00  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "2023-09-17 09:15:00  0.098039  0.098039  0.098039  0.054902  0.054902   \n",
              "2023-09-17 19:30:00  0.082353  0.113725  0.074510  0.019608  0.047059   \n",
              "2023-09-17 23:00:00  0.109804  0.109804  0.109804  0.019608  0.019608   \n",
              "2023-09-17 10:30:00  0.070588  0.070588  0.070588  0.047059  0.047059   \n",
              "...                       ...       ...       ...       ...       ...   \n",
              "2023-09-17 14:15:00  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "2023-09-17 20:45:00  0.098039  0.149020  0.094118  0.011765  0.062745   \n",
              "2023-09-17 10:15:00  0.086275  0.086275  0.086275  0.062745  0.062745   \n",
              "2023-09-17 02:00:00  0.133333  0.133333  0.133333  0.043137  0.043137   \n",
              "2023-09-17 10:45:00  0.050980  0.050980  0.050980  0.035294  0.035294   \n",
              "\n",
              "                       5         6         7         8         9       ...  \\\n",
              "2023-09-17 12:00:00  0.000000  0.000000  0.000000  0.000000  0.000000  ...   \n",
              "2023-09-17 09:15:00  0.054902  0.070588  0.070588  0.070588  0.086275  ...   \n",
              "2023-09-17 19:30:00  0.011765  0.043137  0.074510  0.035294  0.074510  ...   \n",
              "2023-09-17 23:00:00  0.019608  0.043137  0.043137  0.043137  0.086275  ...   \n",
              "2023-09-17 10:30:00  0.047059  0.047059  0.047059  0.047059  0.047059  ...   \n",
              "...                       ...       ...       ...       ...       ...  ...   \n",
              "2023-09-17 14:15:00  0.000000  0.000000  0.000000  0.000000  0.000000  ...   \n",
              "2023-09-17 20:45:00  0.007843  0.047059  0.101961  0.047059  0.078431  ...   \n",
              "2023-09-17 10:15:00  0.062745  0.062745  0.062745  0.062745  0.062745  ...   \n",
              "2023-09-17 02:00:00  0.043137  0.062745  0.062745  0.062745  0.105882  ...   \n",
              "2023-09-17 10:45:00  0.035294  0.043137  0.043137  0.043137  0.015686  ...   \n",
              "\n",
              "                       196598    196599    196600    196601    196602  \\\n",
              "2023-09-17 12:00:00  0.666667  0.654902  0.654902  0.654902  0.580392   \n",
              "2023-09-17 09:15:00  0.541176  0.431373  0.431373  0.431373  0.176471   \n",
              "2023-09-17 19:30:00  0.286275  0.372549  0.396078  0.376471  0.407843   \n",
              "2023-09-17 23:00:00  0.552941  0.286275  0.286275  0.286275  0.274510   \n",
              "2023-09-17 10:30:00  0.549020  0.478431  0.478431  0.478431  0.478431   \n",
              "...                       ...       ...       ...       ...       ...   \n",
              "2023-09-17 14:15:00  0.274510  0.752941  0.717647  0.670588  0.345098   \n",
              "2023-09-17 20:45:00  0.168627  0.137255  0.188235  0.133333  0.156863   \n",
              "2023-09-17 10:15:00  0.639216  0.525490  0.525490  0.525490  0.415686   \n",
              "2023-09-17 02:00:00  0.549020  0.498039  0.498039  0.498039  0.560784   \n",
              "2023-09-17 10:45:00  0.560784  0.537255  0.537255  0.537255  0.486275   \n",
              "\n",
              "                       196603    196604    196605    196606    196607  \n",
              "2023-09-17 12:00:00  0.580392  0.580392  0.650980  0.650980  0.650980  \n",
              "2023-09-17 09:15:00  0.176471  0.176471  0.517647  0.517647  0.517647  \n",
              "2023-09-17 19:30:00  0.431373  0.411765  0.356863  0.380392  0.360784  \n",
              "2023-09-17 23:00:00  0.274510  0.274510  0.149020  0.149020  0.149020  \n",
              "2023-09-17 10:30:00  0.478431  0.478431  0.403922  0.403922  0.403922  \n",
              "...                       ...       ...       ...       ...       ...  \n",
              "2023-09-17 14:15:00  0.352941  0.352941  0.333333  0.341176  0.345098  \n",
              "2023-09-17 20:45:00  0.207843  0.152941  0.141176  0.192157  0.137255  \n",
              "2023-09-17 10:15:00  0.415686  0.415686  0.270588  0.270588  0.270588  \n",
              "2023-09-17 02:00:00  0.560784  0.560784  0.705882  0.705882  0.705882  \n",
              "2023-09-17 10:45:00  0.486275  0.486275  0.588235  0.588235  0.588235  \n",
              "\n",
              "[88 rows x 196608 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c30b6ee-430f-4309-9586-6f063a09bb6c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>196598</th>\n",
              "      <th>196599</th>\n",
              "      <th>196600</th>\n",
              "      <th>196601</th>\n",
              "      <th>196602</th>\n",
              "      <th>196603</th>\n",
              "      <th>196604</th>\n",
              "      <th>196605</th>\n",
              "      <th>196606</th>\n",
              "      <th>196607</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-09-17 12:00:00</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.580392</td>\n",
              "      <td>0.580392</td>\n",
              "      <td>0.580392</td>\n",
              "      <td>0.650980</td>\n",
              "      <td>0.650980</td>\n",
              "      <td>0.650980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 09:15:00</th>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.541176</td>\n",
              "      <td>0.431373</td>\n",
              "      <td>0.431373</td>\n",
              "      <td>0.431373</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.517647</td>\n",
              "      <td>0.517647</td>\n",
              "      <td>0.517647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 19:30:00</th>\n",
              "      <td>0.082353</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>0.372549</td>\n",
              "      <td>0.396078</td>\n",
              "      <td>0.376471</td>\n",
              "      <td>0.407843</td>\n",
              "      <td>0.431373</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.356863</td>\n",
              "      <td>0.380392</td>\n",
              "      <td>0.360784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 23:00:00</th>\n",
              "      <td>0.109804</td>\n",
              "      <td>0.109804</td>\n",
              "      <td>0.109804</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.149020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 10:30:00</th>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.403922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 14:15:00</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.345098</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.341176</td>\n",
              "      <td>0.345098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 20:45:00</th>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.094118</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.101961</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.078431</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168627</td>\n",
              "      <td>0.137255</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.156863</td>\n",
              "      <td>0.207843</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>0.141176</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>0.137255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 10:15:00</th>\n",
              "      <td>0.086275</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>...</td>\n",
              "      <td>0.639216</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>0.415686</td>\n",
              "      <td>0.415686</td>\n",
              "      <td>0.415686</td>\n",
              "      <td>0.270588</td>\n",
              "      <td>0.270588</td>\n",
              "      <td>0.270588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 02:00:00</th>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.105882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.705882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-09-17 10:45:00</th>\n",
              "      <td>0.050980</td>\n",
              "      <td>0.050980</td>\n",
              "      <td>0.050980</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.015686</td>\n",
              "      <td>...</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.537255</td>\n",
              "      <td>0.537255</td>\n",
              "      <td>0.537255</td>\n",
              "      <td>0.486275</td>\n",
              "      <td>0.486275</td>\n",
              "      <td>0.486275</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.588235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88 rows Ã— 196608 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c30b6ee-430f-4309-9586-6f063a09bb6c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c30b6ee-430f-4309-9586-6f063a09bb6c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c30b6ee-430f-4309-9586-6f063a09bb6c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a492d9f-2fb6-479c-aa65-9457bcbfa595\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a492d9f-2fb6-479c-aa65-9457bcbfa595')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a492d9f-2fb6-479c-aa65-9457bcbfa595 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_79ebb04f-9cd0-408d-a3e5-b171c33b0f6b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_79ebb04f-9cd0-408d-a3e5-b171c33b0f6b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Define the number of random images to display\n",
        "num_images_to_display = 5  # You can change this number as needed\n",
        "\n",
        "# Get a random sample of timestamps\n",
        "random_timestamps = random.sample(list(df.index), num_images_to_display)\n",
        "\n",
        "# Plot and display the random images\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, timestamp in enumerate(random_timestamps):\n",
        "    plt.subplot(1, num_images_to_display, i + 1)\n",
        "    image_data = df.loc[timestamp].values.reshape(256, 256, -1)  # Reshape to 256x256x4 (RGBA)\n",
        "    image_data = (image_data * 255).astype(np.uint8)  # Convert back to uint8\n",
        "    plt.imshow(cv2.cvtColor(image_data, cv2.COLOR_RGBA2RGB))  # Convert RGBA to RGB\n",
        "    plt.title(timestamp)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eDJ59R4NvflA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming df is the DataFrame that you have loaded with the data you showed.\n",
        "# df = pd.read_csv('your_file.csv', index_col=0)\n",
        "\n",
        "# Now let's reshape the data back into image format assuming the images are grayscale\n",
        "# If they are color, you would need to divide the number of columns by 3 and stack the channels accordingly\n",
        "image_width = image_height = int((df.shape[1] // 3) ** 0.5)  # example for square RGB images\n",
        "images = df.values.reshape(-1, image_width, image_height, 3)\n",
        "\n",
        "# Split the data into training and validation sets (e.g., 80-20 split)\n",
        "X_train, X_val = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "# X_train now contains your training images, and X_val contains your validation images.\n"
      ],
      "metadata": {
        "id": "V-A4Cz1aEw0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.backend import clear_session\n",
        "clear_session()\n",
        "\n"
      ],
      "metadata": {
        "id": "fUpusH5aHckM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization, LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set the image dimensions and latent dimension\n",
        "img_rows = 256\n",
        "img_cols = 256\n",
        "channels = 3\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "latent_dim = 1000\n",
        "\n",
        "# Build and compile the discriminator\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "    return Model(img, validity)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "    model.summary()\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    img = model(noise)\n",
        "    return Model(noise, img)\n",
        "\n",
        "# The generator takes noise as input and generates images\n",
        "z = Input(shape=(latent_dim,))\n",
        "generator = build_generator()\n",
        "img = generator(z)\n",
        "\n",
        "# Now build the optimizer with the combined model's variables\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "optimizer.build(generator.trainable_variables)\n",
        "\n",
        "# Build and compile the combined model\n",
        "valid = discriminator(img)\n",
        "discriminator.trainable = False  # Ensure this is done before the combined model is compiled\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "# Training hyperparameters\n",
        "epochs = 10000\n",
        "batch_size = 32\n",
        "sample_interval = 200\n",
        "\n",
        "# Adversarial ground truths\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyQMHhIuKD2T",
        "outputId": "85713ae2-d40e-44db-9d94-709fab81bea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 196608)            0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               100663808 \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100795393 (384.50 MB)\n",
            "Trainable params: 100795393 (384.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 256)               256256    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 1024)              4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 196608)            201523200 \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 256, 256, 3)       0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202443520 (772.26 MB)\n",
            "Trainable params: 202439936 (772.25 MB)\n",
            "Non-trainable params: 3584 (14.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random batch of images\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    # Sample noise and generate a batch of new images\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator (real classified as ones and generated as zeros)\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    # Sample noise and generate a batch of new images\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # The generator wants the discriminator to label the generated samples as valid\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
        "\n",
        "    # If at save interval => save generated image samples and perform validation\n",
        "    if epoch % sample_interval == 0:\n",
        "        # Save generated images for validation purposes\n",
        "        # save_images(epoch, generator)\n",
        "\n",
        "        # Evaluate discriminator performance on the validation set\n",
        "        idx = np.random.randint(0, X_val.shape[0], batch_size)\n",
        "        imgs_val = X_val[idx]\n",
        "        d_loss_val = discriminator.evaluate(imgs_val, valid, verbose=0)\n",
        "        print(f\"{epoch} [Validation loss: {d_loss_val[0]}, acc.: {100*d_loss_val[1]:.2f}%]\")\n",
        "\n",
        "        # Optionally, save model checkpoints\n",
        "        # generator.save(f'generator_{epoch}.h5')\n",
        "        # discriminator.save(f'discriminator_{epoch}.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ChhSiF3mId0V",
        "outputId": "5baee1da-a78a-4e1c-f6bc-d533585fe93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 112ms/step\n",
            "0 [D loss: 0.6767045855522156, acc.: 78.12%] [G loss: 0.751366913318634]\n",
            "0 [Validation loss: 0.0, acc.: 100.00%]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1 [D loss: 0.5116108059883118, acc.: 60.94%] [G loss: 1.3375217914581299]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "2 [D loss: 0.21510858833789825, acc.: 89.06%] [G loss: 2.1920266151428223]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "3 [D loss: 0.20564652979373932, acc.: 90.62%] [G loss: 2.8799705505371094]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "4 [D loss: 0.22779160737991333, acc.: 90.62%] [G loss: 4.126317977905273]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "5 [D loss: 0.4154391288757324, acc.: 81.25%] [G loss: 3.0297069549560547]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "6 [D loss: 0.42381978034973145, acc.: 81.25%] [G loss: 3.70058012008667]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "7 [D loss: 0.3006537854671478, acc.: 87.50%] [G loss: 3.757883310317993]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "8 [D loss: 0.9554442167282104, acc.: 76.56%] [G loss: 4.5776214599609375]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "9 [D loss: 2.14878511428833, acc.: 67.19%] [G loss: 4.51131534576416]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "10 [D loss: 0.8195801377296448, acc.: 79.69%] [G loss: 4.535862922668457]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "11 [D loss: 1.3771986961364746, acc.: 79.69%] [G loss: 5.568489074707031]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "12 [D loss: 1.7002406120300293, acc.: 71.88%] [G loss: 4.918951988220215]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "13 [D loss: 0.9737595319747925, acc.: 79.69%] [G loss: 3.992828369140625]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "14 [D loss: 1.20769464969635, acc.: 73.44%] [G loss: 4.685426712036133]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "15 [D loss: 0.45834916830062866, acc.: 85.94%] [G loss: 4.42856502532959]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "16 [D loss: 0.26339155442787104, acc.: 92.19%] [G loss: 4.378890037536621]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "17 [D loss: 11.804561138153076, acc.: 21.88%] [G loss: 5.7550554275512695]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "18 [D loss: 3.0031380653381348, acc.: 79.69%] [G loss: 8.680425643920898]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "19 [D loss: 3.594493865966797, acc.: 71.88%] [G loss: 13.935660362243652]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "20 [D loss: 5.36143684387207, acc.: 70.31%] [G loss: 8.315315246582031]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "21 [D loss: 3.6818597316741943, acc.: 79.69%] [G loss: 14.410861015319824]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "22 [D loss: 1.9327561855316162, acc.: 81.25%] [G loss: 13.519537925720215]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "23 [D loss: 1.4081177711486816, acc.: 81.25%] [G loss: 9.95941162109375]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "24 [D loss: 1.0844581127166748, acc.: 81.25%] [G loss: 13.108564376831055]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "25 [D loss: 0.21828430945383176, acc.: 90.62%] [G loss: 13.980525970458984]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26 [D loss: 42.16459798812866, acc.: 23.44%] [G loss: 21.210643768310547]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "27 [D loss: 10.3258695602417, acc.: 71.88%] [G loss: 20.941913604736328]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "28 [D loss: 16.72536277770996, acc.: 67.19%] [G loss: 23.715930938720703]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "29 [D loss: 5.439008712768555, acc.: 81.25%] [G loss: 27.419431686401367]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "30 [D loss: 17.262147903442383, acc.: 71.88%] [G loss: 23.140365600585938]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "31 [D loss: 17.53607940673828, acc.: 70.31%] [G loss: 25.817596435546875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "32 [D loss: 9.067094802856445, acc.: 78.12%] [G loss: 28.589649200439453]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "33 [D loss: 7.916712760925293, acc.: 79.69%] [G loss: 30.065908432006836]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "34 [D loss: 11.179433822631836, acc.: 73.44%] [G loss: 20.25983428955078]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "35 [D loss: 5.610766410827637, acc.: 73.44%] [G loss: 18.116710662841797]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "36 [D loss: 7.8052167892456055, acc.: 78.12%] [G loss: 24.570369720458984]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "37 [D loss: 7.256819725036621, acc.: 73.44%] [G loss: 14.599227905273438]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "38 [D loss: 3.99998140335083, acc.: 81.25%] [G loss: 18.30931854248047]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "39 [D loss: 7.135970592498779, acc.: 67.19%] [G loss: 10.561058044433594]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "40 [D loss: 3.582338809967041, acc.: 76.56%] [G loss: 6.446170806884766]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "41 [D loss: 1.4053759574890137, acc.: 76.56%] [G loss: 11.589990615844727]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "42 [D loss: 1.396443486213684, acc.: 78.12%] [G loss: 9.840189933776855]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "43 [D loss: 0.6844506785273552, acc.: 87.50%] [G loss: 11.547752380371094]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "44 [D loss: 0.8874514698982239, acc.: 79.69%] [G loss: 14.107122421264648]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "45 [D loss: 0.21374356746673587, acc.: 92.19%] [G loss: 10.046005249023438]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "46 [D loss: 2.4004276990890503, acc.: 76.56%] [G loss: 6.351901054382324]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "47 [D loss: 1.8985815048217773, acc.: 78.12%] [G loss: 5.451797008514404]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "48 [D loss: 2.7733304500579834, acc.: 70.31%] [G loss: 9.176080703735352]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "49 [D loss: 2.313976287841797, acc.: 79.69%] [G loss: 7.484213829040527]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "50 [D loss: 0.46339350938796997, acc.: 93.75%] [G loss: 8.38608169555664]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "51 [D loss: 0.9138305187225342, acc.: 84.38%] [G loss: 11.415613174438477]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "52 [D loss: 1.2540594935417175, acc.: 82.81%] [G loss: 9.67212963104248]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "53 [D loss: 0.6893908977508545, acc.: 84.38%] [G loss: 10.962600708007812]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "54 [D loss: 0.4322887361050332, acc.: 93.75%] [G loss: 9.034027099609375]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "55 [D loss: 1.8967171758413315, acc.: 75.00%] [G loss: 6.018869876861572]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "56 [D loss: 2.9951698780059814, acc.: 68.75%] [G loss: 6.297027587890625]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "57 [D loss: 1.9760159254074097, acc.: 75.00%] [G loss: 9.121199607849121]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "58 [D loss: 0.7284560203552246, acc.: 82.81%] [G loss: 6.307007312774658]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "59 [D loss: 0.6921285390853882, acc.: 84.38%] [G loss: 7.541408538818359]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "60 [D loss: 0.6859122514724731, acc.: 82.81%] [G loss: 8.322601318359375]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "61 [D loss: 1.6542056798934937, acc.: 82.81%] [G loss: 6.593532562255859]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "62 [D loss: 0.7081037759780884, acc.: 81.25%] [G loss: 8.75460433959961]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "63 [D loss: 0.18954327719614583, acc.: 93.75%] [G loss: 6.306466102600098]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "64 [D loss: 10.098363637924194, acc.: 21.88%] [G loss: 4.297686576843262]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "65 [D loss: 7.93284797668457, acc.: 73.44%] [G loss: 5.586454391479492]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "66 [D loss: 13.669092178344727, acc.: 67.19%] [G loss: 12.789752960205078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "67 [D loss: 12.654178619384766, acc.: 67.19%] [G loss: 10.417773246765137]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "68 [D loss: 11.042736053466797, acc.: 70.31%] [G loss: 18.689462661743164]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "69 [D loss: 11.057307243347168, acc.: 68.75%] [G loss: 18.300792694091797]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "70 [D loss: 17.374433517456055, acc.: 62.50%] [G loss: 14.978371620178223]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "71 [D loss: 9.578461647033691, acc.: 71.88%] [G loss: 15.095632553100586]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "72 [D loss: 9.812921524047852, acc.: 65.62%] [G loss: 19.72144889831543]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "73 [D loss: 4.767651557922363, acc.: 76.56%] [G loss: 13.31610107421875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "74 [D loss: 4.532019138336182, acc.: 70.31%] [G loss: 24.37570571899414]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "75 [D loss: 1.7704108953475952, acc.: 76.56%] [G loss: 15.592613220214844]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "76 [D loss: 0.9054322242736834, acc.: 84.38%] [G loss: 15.429193496704102]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "77 [D loss: 0.5471006898009243, acc.: 84.38%] [G loss: 18.884265899658203]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "78 [D loss: 20.137734413146973, acc.: 20.31%] [G loss: 7.533998489379883]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "79 [D loss: 16.657787322998047, acc.: 64.06%] [G loss: 21.90988540649414]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "80 [D loss: 13.411848068237305, acc.: 67.19%] [G loss: 19.418094635009766]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "81 [D loss: 20.300994873046875, acc.: 60.94%] [G loss: 31.001571655273438]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "82 [D loss: 18.313488006591797, acc.: 65.62%] [G loss: 19.4731388092041]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "83 [D loss: 13.245698928833008, acc.: 68.75%] [G loss: 19.203128814697266]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "84 [D loss: 17.06344223022461, acc.: 62.50%] [G loss: 19.611011505126953]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "85 [D loss: 15.037393569946289, acc.: 65.62%] [G loss: 11.151517868041992]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "86 [D loss: 8.950774192810059, acc.: 59.38%] [G loss: 16.976520538330078]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "87 [D loss: 4.069513320922852, acc.: 70.31%] [G loss: 25.045875549316406]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "88 [D loss: 2.9638757705688477, acc.: 73.44%] [G loss: 14.323737144470215]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "89 [D loss: 0.5851495862007141, acc.: 87.50%] [G loss: 16.49274444580078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "90 [D loss: 0.3724278033624273, acc.: 93.75%] [G loss: 9.134448051452637]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "91 [D loss: 11.596176147460938, acc.: 50.00%] [G loss: 20.581518173217773]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "92 [D loss: 8.284713745117188, acc.: 68.75%] [G loss: 9.755183219909668]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "93 [D loss: 8.094980239868164, acc.: 70.31%] [G loss: 10.200101852416992]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "94 [D loss: 5.270709037780762, acc.: 75.00%] [G loss: 11.003278732299805]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "95 [D loss: 6.006805896759033, acc.: 67.19%] [G loss: 17.025005340576172]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "96 [D loss: 5.211203575134277, acc.: 70.31%] [G loss: 11.686885833740234]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "97 [D loss: 4.192082405090332, acc.: 67.19%] [G loss: 10.010629653930664]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "98 [D loss: 1.3040042737441553, acc.: 84.38%] [G loss: 18.333106994628906]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "99 [D loss: 0.329238623380661, acc.: 93.75%] [G loss: 12.639241218566895]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "100 [D loss: 2.7283172607421875, acc.: 81.25%] [G loss: 8.729840278625488]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "101 [D loss: 2.5235445499420166, acc.: 76.56%] [G loss: 15.331802368164062]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "102 [D loss: 1.2866073846817017, acc.: 87.50%] [G loss: 9.316624641418457]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "103 [D loss: 3.9789844751358032, acc.: 68.75%] [G loss: 12.956249237060547]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "104 [D loss: 4.388742446899414, acc.: 67.19%] [G loss: 5.994123458862305]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "105 [D loss: 2.6473186016082764, acc.: 76.56%] [G loss: 6.985751152038574]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "106 [D loss: 0.6491089463233948, acc.: 84.38%] [G loss: 6.428055763244629]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "107 [D loss: 0.7287616033718223, acc.: 87.50%] [G loss: 14.118478775024414]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "108 [D loss: 0.6154875159263611, acc.: 85.94%] [G loss: 10.148761749267578]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "109 [D loss: 0.04317507892847328, acc.: 98.44%] [G loss: 9.232898712158203]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "110 [D loss: 0.7795189641874458, acc.: 85.94%] [G loss: 10.460480690002441]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "111 [D loss: 12.291292190551758, acc.: 32.81%] [G loss: 4.134547233581543]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "112 [D loss: 14.692277908325195, acc.: 57.81%] [G loss: 6.672098159790039]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "113 [D loss: 12.656916618347168, acc.: 59.38%] [G loss: 3.7985787391662598]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "114 [D loss: 9.629070281982422, acc.: 64.06%] [G loss: 14.750858306884766]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "115 [D loss: 11.077832221984863, acc.: 65.62%] [G loss: 13.27093505859375]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "116 [D loss: 7.941932201385498, acc.: 67.19%] [G loss: 13.84506607055664]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "117 [D loss: 5.94063663482666, acc.: 67.19%] [G loss: 11.010485649108887]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "118 [D loss: 9.058084487915039, acc.: 67.19%] [G loss: 11.091217041015625]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "119 [D loss: 5.292124271392822, acc.: 59.38%] [G loss: 10.503507614135742]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "120 [D loss: 1.9457850456237793, acc.: 78.12%] [G loss: 9.220308303833008]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "121 [D loss: 1.885704517364502, acc.: 82.81%] [G loss: 8.220168113708496]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "122 [D loss: 0.9247012138366699, acc.: 87.50%] [G loss: 7.9255595207214355]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "123 [D loss: 1.2030019760131836, acc.: 84.38%] [G loss: 13.96092700958252]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "124 [D loss: 1.1750231981277466, acc.: 81.25%] [G loss: 15.155509948730469]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "125 [D loss: 0.12279583527444726, acc.: 95.31%] [G loss: 16.00606918334961]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "126 [D loss: 7.209096908569336, acc.: 56.25%] [G loss: 16.7998046875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "127 [D loss: 7.994269371032715, acc.: 64.06%] [G loss: 10.115194320678711]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "128 [D loss: 8.209856033325195, acc.: 67.19%] [G loss: 7.6384735107421875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "129 [D loss: 8.550676345825195, acc.: 64.06%] [G loss: 6.014585494995117]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "130 [D loss: 6.750949382781982, acc.: 60.94%] [G loss: 9.519022941589355]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "131 [D loss: 3.5451650619506836, acc.: 70.31%] [G loss: 10.232025146484375]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "132 [D loss: 1.7864965200424194, acc.: 76.56%] [G loss: 14.923412322998047]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "133 [D loss: 1.1755742476689193, acc.: 78.12%] [G loss: 13.490457534790039]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "134 [D loss: 0.3001778721809387, acc.: 93.75%] [G loss: 13.824480056762695]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "135 [D loss: 0.4652068123805293, acc.: 87.50%] [G loss: 15.105769157409668]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "136 [D loss: 3.7858335971832275, acc.: 59.38%] [G loss: 10.09752082824707]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "137 [D loss: 6.552613258361816, acc.: 67.19%] [G loss: 9.563812255859375]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "138 [D loss: 6.94664192199707, acc.: 67.19%] [G loss: 2.851593494415283]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "139 [D loss: 5.171805381774902, acc.: 64.06%] [G loss: 8.277524948120117]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "140 [D loss: 3.842066526412964, acc.: 76.56%] [G loss: 10.490575790405273]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "141 [D loss: 4.1799468994140625, acc.: 70.31%] [G loss: 16.738737106323242]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "142 [D loss: 2.0537514686584473, acc.: 76.56%] [G loss: 15.995756149291992]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "143 [D loss: 1.6395092010498047, acc.: 82.81%] [G loss: 18.442787170410156]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "144 [D loss: 1.4109294627360214, acc.: 89.06%] [G loss: 11.07919692993164]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "145 [D loss: 0.8225606679916382, acc.: 84.38%] [G loss: 13.078453063964844]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "146 [D loss: 1.7893568873405457, acc.: 81.25%] [G loss: 8.257705688476562]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "147 [D loss: 2.1952473521232605, acc.: 76.56%] [G loss: 13.85951042175293]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "148 [D loss: 0.8606094717979431, acc.: 89.06%] [G loss: 11.706499099731445]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "149 [D loss: 0.46131786704063416, acc.: 87.50%] [G loss: 17.88423728942871]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "150 [D loss: 0.6894634962081909, acc.: 87.50%] [G loss: 12.792304992675781]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "151 [D loss: 0.980355978012085, acc.: 90.62%] [G loss: 12.040184020996094]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "152 [D loss: 0.6723669171333474, acc.: 87.50%] [G loss: 12.668787002563477]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "153 [D loss: 1.0871764421463013, acc.: 87.50%] [G loss: 16.780231475830078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "154 [D loss: 0.5920492101240598, acc.: 89.06%] [G loss: 17.886716842651367]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "155 [D loss: 8.07828950881958, acc.: 37.50%] [G loss: 4.342618942260742]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "156 [D loss: 9.242347717285156, acc.: 57.81%] [G loss: 6.307010650634766]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "157 [D loss: 7.696134567260742, acc.: 65.62%] [G loss: 9.551794052124023]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "158 [D loss: 5.939965724945068, acc.: 70.31%] [G loss: 15.858522415161133]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "159 [D loss: 5.27659797668457, acc.: 65.62%] [G loss: 5.389798164367676]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "160 [D loss: 4.850354194641113, acc.: 71.88%] [G loss: 14.77450942993164]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "161 [D loss: 1.4884328842163086, acc.: 84.38%] [G loss: 12.82718276977539]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "162 [D loss: 2.6081995964050293, acc.: 78.12%] [G loss: 17.561296463012695]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "163 [D loss: 1.0389021613373188, acc.: 87.50%] [G loss: 12.64974594116211]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "164 [D loss: 2.0260384678840637, acc.: 79.69%] [G loss: 14.644225120544434]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "165 [D loss: 0.9607302130708799, acc.: 82.81%] [G loss: 9.33425235748291]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "166 [D loss: 0.5496822595596313, acc.: 89.06%] [G loss: 28.130281448364258]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "167 [D loss: 0.43971288205031955, acc.: 85.94%] [G loss: 18.568321228027344]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "168 [D loss: 10.825562000274658, acc.: 42.19%] [G loss: 7.076627731323242]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "169 [D loss: 10.346558570861816, acc.: 65.62%] [G loss: 10.979193687438965]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "170 [D loss: 8.894974708557129, acc.: 65.62%] [G loss: 12.190818786621094]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "171 [D loss: 7.391959190368652, acc.: 67.19%] [G loss: 13.0537109375]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "172 [D loss: 7.690394401550293, acc.: 65.62%] [G loss: 13.564401626586914]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "173 [D loss: 3.039914131164551, acc.: 79.69%] [G loss: 14.304439544677734]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "174 [D loss: 1.948728322982788, acc.: 79.69%] [G loss: 13.104097366333008]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "175 [D loss: 1.450632333780065, acc.: 81.25%] [G loss: 18.26324462890625]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "176 [D loss: 0.4602951109409491, acc.: 89.06%] [G loss: 24.861812591552734]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "177 [D loss: 4.628337860107422, acc.: 54.69%] [G loss: 19.38087272644043]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "178 [D loss: 8.281261444091797, acc.: 68.75%] [G loss: 10.772632598876953]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "179 [D loss: 5.938480377197266, acc.: 68.75%] [G loss: 12.266093254089355]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "180 [D loss: 3.8449394702911377, acc.: 70.31%] [G loss: 19.39655303955078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "181 [D loss: 3.0050601959228516, acc.: 76.56%] [G loss: 13.74880313873291]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "182 [D loss: 1.0740374326705933, acc.: 84.38%] [G loss: 17.43857192993164]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "183 [D loss: 0.6348488330841064, acc.: 92.19%] [G loss: 25.419620513916016]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "184 [D loss: 2.3579486906528473, acc.: 75.00%] [G loss: 22.76705551147461]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "185 [D loss: 1.0545437335968018, acc.: 84.38%] [G loss: 24.367332458496094]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "186 [D loss: 0.5297306776046753, acc.: 87.50%] [G loss: 22.149009704589844]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "187 [D loss: 0.011684426106512548, acc.: 100.00%] [G loss: 22.189876556396484]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "188 [D loss: 0.5593575313687325, acc.: 93.75%] [G loss: 27.89749526977539]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "189 [D loss: 0.7126065492630005, acc.: 85.94%] [G loss: 21.966590881347656]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "190 [D loss: 0.2057177630806466, acc.: 93.75%] [G loss: 23.51818084716797]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "191 [D loss: 0.38755956292152405, acc.: 93.75%] [G loss: 22.791744232177734]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "192 [D loss: 0.1791068017482765, acc.: 95.31%] [G loss: 25.19481658935547]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "193 [D loss: 0.13930041688921335, acc.: 96.88%] [G loss: 27.186084747314453]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "194 [D loss: 1.5855101943016052, acc.: 76.56%] [G loss: 14.73599910736084]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "195 [D loss: 2.101935863494873, acc.: 79.69%] [G loss: 11.13841724395752]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "196 [D loss: 2.0353615283966064, acc.: 79.69%] [G loss: 15.275161743164062]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "197 [D loss: 0.719417929649353, acc.: 85.94%] [G loss: 15.358736038208008]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "198 [D loss: 0.2513922154903412, acc.: 92.19%] [G loss: 13.103487968444824]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "199 [D loss: 0.258672833442688, acc.: 92.19%] [G loss: 22.148807525634766]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "200 [D loss: 0.6668010652065277, acc.: 90.62%] [G loss: 16.596355438232422]\n",
            "200 [Validation loss: 8.602077824191885e-15, acc.: 100.00%]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "201 [D loss: 0.2302114367485838, acc.: 93.75%] [G loss: 21.144962310791016]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "202 [D loss: 0.1338136378118584, acc.: 95.31%] [G loss: 21.330692291259766]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "203 [D loss: 0.6935276240110397, acc.: 78.12%] [G loss: 13.031665802001953]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "204 [D loss: 2.0545437335968018, acc.: 81.25%] [G loss: 18.225923538208008]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "205 [D loss: 1.3043830394744873, acc.: 81.25%] [G loss: 10.770050048828125]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "206 [D loss: 0.3627271354198461, acc.: 89.06%] [G loss: 16.910831451416016]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "207 [D loss: 1.4797227084636688, acc.: 95.31%] [G loss: 15.788742065429688]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "208 [D loss: 0.29309260845184326, acc.: 90.62%] [G loss: 16.68829345703125]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "209 [D loss: 0.8730341792106628, acc.: 87.50%] [G loss: 16.08922004699707]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "210 [D loss: 0.36440391871929023, acc.: 93.75%] [G loss: 17.507123947143555]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "211 [D loss: 1.8155162334442139, acc.: 79.69%] [G loss: 8.468887329101562]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "212 [D loss: 2.112539052963257, acc.: 79.69%] [G loss: 16.10616683959961]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "213 [D loss: 1.7120413780212402, acc.: 79.69%] [G loss: 20.815841674804688]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "214 [D loss: 0.5420088768005864, acc.: 87.50%] [G loss: 12.741578102111816]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "215 [D loss: 1.0579533278942108, acc.: 73.44%] [G loss: 17.31531524658203]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "216 [D loss: 0.18904510140419029, acc.: 93.75%] [G loss: 10.39594841003418]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "217 [D loss: 0.12105050470436929, acc.: 93.75%] [G loss: 17.57320213317871]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "218 [D loss: 3.2033644318580627, acc.: 62.50%] [G loss: 10.631856918334961]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "219 [D loss: 3.1074178218841553, acc.: 73.44%] [G loss: 7.2430620193481445]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "220 [D loss: 2.393864631652832, acc.: 79.69%] [G loss: 9.192412376403809]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "221 [D loss: 0.552656888961792, acc.: 92.19%] [G loss: 15.623160362243652]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "222 [D loss: 1.1485061645507812, acc.: 82.81%] [G loss: 12.20631217956543]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "223 [D loss: 0.09053837510434704, acc.: 96.88%] [G loss: 16.206512451171875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "224 [D loss: 4.894476622343063, acc.: 43.75%] [G loss: 3.172976016998291]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "225 [D loss: 8.572744369506836, acc.: 62.50%] [G loss: 9.453652381896973]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "226 [D loss: 4.24179220199585, acc.: 75.00%] [G loss: 8.864152908325195]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "227 [D loss: 3.6986310482025146, acc.: 70.31%] [G loss: 5.237577438354492]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "228 [D loss: 1.1261868476867676, acc.: 82.81%] [G loss: 16.63720703125]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "229 [D loss: 0.7628598213195801, acc.: 79.69%] [G loss: 12.367973327636719]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "230 [D loss: 0.6104726791381836, acc.: 85.94%] [G loss: 14.830078125]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "231 [D loss: 0.005402515667847092, acc.: 100.00%] [G loss: 20.73858642578125]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "232 [D loss: 0.3546079471707344, acc.: 92.19%] [G loss: 12.81683349609375]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "233 [D loss: 0.48111259937286377, acc.: 95.31%] [G loss: 13.235269546508789]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "234 [D loss: 0.3476900232490152, acc.: 92.19%] [G loss: 13.529101371765137]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "235 [D loss: 0.3289004862308502, acc.: 92.19%] [G loss: 12.14494514465332]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "236 [D loss: 0.11515267334834789, acc.: 96.88%] [G loss: 15.504974365234375]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "237 [D loss: 0.047639147378504276, acc.: 96.88%] [G loss: 9.129647254943848]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "238 [D loss: 0.059502776815136826, acc.: 98.44%] [G loss: 14.031822204589844]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "239 [D loss: 0.09666813164949417, acc.: 96.88%] [G loss: 17.630979537963867]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "240 [D loss: 9.631356239318848, acc.: 26.56%] [G loss: 4.173309326171875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "241 [D loss: 16.521747589111328, acc.: 59.38%] [G loss: 1.4007400274276733]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "242 [D loss: 10.719966888427734, acc.: 59.38%] [G loss: 3.77956485748291]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "243 [D loss: 6.793307304382324, acc.: 68.75%] [G loss: 7.9816718101501465]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "244 [D loss: 3.6947317123413086, acc.: 75.00%] [G loss: 5.614064693450928]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "245 [D loss: 1.6123533248901367, acc.: 78.12%] [G loss: 14.416851043701172]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "246 [D loss: 0.042124126477865786, acc.: 96.88%] [G loss: 16.552452087402344]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "247 [D loss: 25.4581937789917, acc.: 9.38%] [G loss: 0.9511196613311768]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "248 [D loss: 23.528717041015625, acc.: 53.12%] [G loss: 0.4137937128543854]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "249 [D loss: 18.815692901611328, acc.: 57.81%] [G loss: 0.19925430417060852]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "250 [D loss: 15.336786270141602, acc.: 56.25%] [G loss: 2.7212116718292236]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "251 [D loss: 12.423398971557617, acc.: 59.38%] [G loss: 3.441951274871826]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "252 [D loss: 7.957649230957031, acc.: 62.50%] [G loss: 5.052104949951172]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "253 [D loss: 3.6453490257263184, acc.: 70.31%] [G loss: 18.76935577392578]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "254 [D loss: 0.04540242254734039, acc.: 98.44%] [G loss: 15.96119213104248]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "255 [D loss: 23.691645622253418, acc.: 18.75%] [G loss: 1.8220692873001099]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "256 [D loss: 22.39617156982422, acc.: 57.81%] [G loss: 2.0883424282073975]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "257 [D loss: 23.786514282226562, acc.: 56.25%] [G loss: 4.552885055541992]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "258 [D loss: 15.431549072265625, acc.: 56.25%] [G loss: 7.195725440979004]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "259 [D loss: 9.128612518310547, acc.: 70.31%] [G loss: 6.572801113128662]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "260 [D loss: 7.394085884094238, acc.: 67.19%] [G loss: 7.595285892486572]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "261 [D loss: 3.7454540729522705, acc.: 71.88%] [G loss: 17.349613189697266]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "262 [D loss: 0.5367155075073242, acc.: 82.81%] [G loss: 15.496820449829102]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "263 [D loss: 0.09091404825452896, acc.: 96.88%] [G loss: 16.415515899658203]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "264 [D loss: 6.077806830406189, acc.: 50.00%] [G loss: 8.059638977050781]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "265 [D loss: 6.623512268066406, acc.: 64.06%] [G loss: 3.4499099254608154]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "266 [D loss: 3.8066773414611816, acc.: 75.00%] [G loss: 16.264732360839844]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "267 [D loss: 2.3125929832458496, acc.: 81.25%] [G loss: 13.905786514282227]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "268 [D loss: 1.1196568012237549, acc.: 85.94%] [G loss: 12.421412467956543]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "269 [D loss: 0.8874815702438354, acc.: 84.38%] [G loss: 15.979734420776367]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "270 [D loss: 0.6370114684104919, acc.: 92.19%] [G loss: 14.633140563964844]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "271 [D loss: 0.7612640261650085, acc.: 93.75%] [G loss: 27.275848388671875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "272 [D loss: 0.27458301186887063, acc.: 95.31%] [G loss: 18.74016571044922]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "273 [D loss: 0.02128310578604342, acc.: 98.44%] [G loss: 17.6973876953125]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "274 [D loss: 0.13801958647673018, acc.: 95.31%] [G loss: 20.79921531677246]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "275 [D loss: 12.373485088348389, acc.: 25.00%] [G loss: 5.174394607543945]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "276 [D loss: 19.362699508666992, acc.: 56.25%] [G loss: 3.8971619606018066]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "277 [D loss: 10.978160858154297, acc.: 64.06%] [G loss: 3.800330400466919]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "278 [D loss: 12.594437599182129, acc.: 62.50%] [G loss: 10.361639022827148]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "279 [D loss: 8.170682907104492, acc.: 65.62%] [G loss: 7.353379249572754]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "280 [D loss: 5.123542785644531, acc.: 68.75%] [G loss: 10.547933578491211]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "281 [D loss: 2.703725376083149, acc.: 79.69%] [G loss: 13.365291595458984]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "282 [D loss: 2.0707497596740723, acc.: 81.25%] [G loss: 21.871524810791016]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "283 [D loss: 1.7374586872756481, acc.: 78.12%] [G loss: 16.84037208557129]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "284 [D loss: 4.094259709119797, acc.: 59.38%] [G loss: 16.904037475585938]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "285 [D loss: 3.5190677642822266, acc.: 70.31%] [G loss: 15.787778854370117]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "286 [D loss: 3.590792222604854, acc.: 70.31%] [G loss: 11.317726135253906]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "287 [D loss: 1.320138156414032, acc.: 89.06%] [G loss: 25.208663940429688]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "288 [D loss: 0.8398418426522615, acc.: 85.94%] [G loss: 26.358356475830078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "289 [D loss: 1.615819275379181, acc.: 73.44%] [G loss: 11.068723678588867]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "290 [D loss: 4.855236053466797, acc.: 71.88%] [G loss: 10.729399681091309]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "291 [D loss: 2.892787218093872, acc.: 70.31%] [G loss: 17.89822006225586]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "292 [D loss: 1.0060644149780273, acc.: 82.81%] [G loss: 21.14745330810547]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "293 [D loss: 0.2675497532056056, acc.: 93.75%] [G loss: 24.253984451293945]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "294 [D loss: 17.87675380706787, acc.: 18.75%] [G loss: 6.536604881286621]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "295 [D loss: 20.355422973632812, acc.: 59.38%] [G loss: 3.5553131103515625]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "296 [D loss: 12.539023399353027, acc.: 57.81%] [G loss: 5.494062423706055]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "297 [D loss: 6.53159236907959, acc.: 68.75%] [G loss: 4.670605659484863]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "298 [D loss: 7.04534912109375, acc.: 65.62%] [G loss: 7.4978179931640625]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "299 [D loss: 5.049199104309082, acc.: 71.88%] [G loss: 18.16226577758789]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "300 [D loss: 0.8323692083367784, acc.: 90.62%] [G loss: 18.865434646606445]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "301 [D loss: 30.42099952697754, acc.: 6.25%] [G loss: 16.081642150878906]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "302 [D loss: 20.676103591918945, acc.: 57.81%] [G loss: 9.133296966552734]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "303 [D loss: 18.47818946838379, acc.: 56.25%] [G loss: 11.807705879211426]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "304 [D loss: 13.786531448364258, acc.: 65.62%] [G loss: 3.050823211669922]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "305 [D loss: 10.363648414611816, acc.: 65.62%] [G loss: 24.36286163330078]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "306 [D loss: 4.580436706542969, acc.: 62.50%] [G loss: 9.32189655303955]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "307 [D loss: 1.9150047302246094, acc.: 79.69%] [G loss: 22.24080467224121]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "308 [D loss: 0.594413907789253, acc.: 87.50%] [G loss: 28.643611907958984]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "309 [D loss: 23.62008285522461, acc.: 10.94%] [G loss: 10.118818283081055]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "310 [D loss: 20.87857437133789, acc.: 56.25%] [G loss: 12.622836112976074]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "311 [D loss: 14.492509841918945, acc.: 64.06%] [G loss: 12.101824760437012]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "312 [D loss: 5.785768985748291, acc.: 78.12%] [G loss: 12.649293899536133]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "313 [D loss: 5.939109802246094, acc.: 67.19%] [G loss: 30.66860580444336]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "314 [D loss: 4.032827377319336, acc.: 73.44%] [G loss: 16.64031982421875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "315 [D loss: 1.1272518634796143, acc.: 82.81%] [G loss: 28.6112060546875]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "316 [D loss: 21.592659950256348, acc.: 17.19%] [G loss: 18.252853393554688]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "317 [D loss: 13.366353988647461, acc.: 57.81%] [G loss: 12.990755081176758]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "318 [D loss: 13.044074058532715, acc.: 62.50%] [G loss: 17.773653030395508]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "319 [D loss: 8.994773864746094, acc.: 67.19%] [G loss: 14.868783950805664]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "320 [D loss: 5.756972789764404, acc.: 73.44%] [G loss: 23.713525772094727]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "321 [D loss: 2.4007949829101562, acc.: 79.69%] [G loss: 37.79418182373047]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "322 [D loss: 1.0728766918182373, acc.: 84.38%] [G loss: 17.66640281677246]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "323 [D loss: 0.1363341948017478, acc.: 96.88%] [G loss: 39.27158737182617]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "324 [D loss: 17.499831199645996, acc.: 20.31%] [G loss: 14.160411834716797]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "325 [D loss: 17.33176040649414, acc.: 67.19%] [G loss: 3.7518932819366455]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "326 [D loss: 14.69417953491211, acc.: 64.06%] [G loss: 14.738351821899414]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "327 [D loss: 6.595721244812012, acc.: 75.00%] [G loss: 28.536861419677734]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "328 [D loss: 5.876472473144531, acc.: 75.00%] [G loss: 22.454639434814453]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "329 [D loss: 5.0724897384643555, acc.: 68.75%] [G loss: 40.60429763793945]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "330 [D loss: 0.7922266721725464, acc.: 90.62%] [G loss: 30.577281951904297]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "331 [D loss: 0.13928565971657036, acc.: 95.31%] [G loss: 34.21235656738281]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "332 [D loss: 20.85743808746338, acc.: 15.62%] [G loss: 25.33769989013672]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "333 [D loss: 17.092628479003906, acc.: 62.50%] [G loss: 11.490861892700195]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "334 [D loss: 16.711496353149414, acc.: 65.62%] [G loss: 13.892058372497559]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "335 [D loss: 8.776214599609375, acc.: 68.75%] [G loss: 14.942907333374023]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "336 [D loss: 6.429993152618408, acc.: 70.31%] [G loss: 9.489543914794922]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "337 [D loss: 6.463885307312012, acc.: 67.19%] [G loss: 20.025732040405273]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "338 [D loss: 3.0129549503326416, acc.: 78.12%] [G loss: 22.287826538085938]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "339 [D loss: 0.5889131563017145, acc.: 89.06%] [G loss: 25.704219818115234]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "340 [D loss: 19.098567008972168, acc.: 10.94%] [G loss: 6.972576141357422]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "341 [D loss: 15.75062370300293, acc.: 60.94%] [G loss: 6.249832630157471]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "342 [D loss: 9.414213180541992, acc.: 62.50%] [G loss: 12.2188720703125]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "343 [D loss: 5.999611854553223, acc.: 73.44%] [G loss: 27.79804229736328]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "344 [D loss: 0.9751765131950378, acc.: 82.81%] [G loss: 25.62876319885254]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-7c37238fd4ed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Sample noise and generate a batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}